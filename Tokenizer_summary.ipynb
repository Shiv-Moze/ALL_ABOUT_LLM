{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChuIwehMUQTX"
   },
   "source": [
    "Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we‚Äôll explore exactly what happens in the tokenization pipeline.\n",
    "\n",
    "In NLP tasks, the data that is generally processed is raw text. Here‚Äôs an example of such text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvpJSqr8U_ur"
   },
   "outputs": [],
   "source": [
    "# \"Jim Henson was a puppeteer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq0nCQz8U8az"
   },
   "source": [
    "However, models can only process numbers, so we need to find a way to convert the raw text to numbers. That‚Äôs what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation ‚Äî that is, the one that makes the most sense to the model ‚Äî and, if possible, the smallest representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGC1fvJ3VLpv"
   },
   "source": [
    "**Word-based(Word Tokenization):Splits text at whitespace or punctuation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nU9Yz6HSW6n0"
   },
   "source": [
    "Word-based tokenizers assign a unique ID to every word, creating large vocabularies (e.g., 500,000+ words in English). This approach struggles with inflected forms like ‚Äúdog‚Äù vs. ‚Äúdogs‚Äù or ‚Äúrun‚Äù vs. ‚Äúrunning,‚Äù treating them as unrelated. It also requires a special [UNK] token for unknown words not in the vocabulary, which leads to information loss if used too often. To reduce unknown tokens and handle rare words better, one alternative is to use character-based tokenization, which breaks words into smaller, more manageable units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHLN0m0_UXen",
    "outputId": "5423a2c3-ccab-4b41-9572-7c6fef2fa079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "# There are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python‚Äôs split() function:\n",
    "\n",
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXjIhgUsYWkP"
   },
   "source": [
    "# Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0zqXPONYbFY"
   },
   "source": [
    "Breaks rare or unknown words into smaller known subword units. Popular in LLMs.\n",
    "\n",
    "Variants:\n",
    "Byte-Pair Encoding (BPE)\n",
    "\n",
    "WordPiece\n",
    "\n",
    "SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzl-iOoGYecY"
   },
   "source": [
    "***a. Byte-Pair Encoding (BPE)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6DqPmzfXBa3"
   },
   "outputs": [],
   "source": [
    "# Used in: GPT, RoBERTa, GPT-Neo, GPT-J\n",
    "# \"unhappiness\" ‚Üí ['un', 'happiness']\n",
    "# \"disappointed\" ‚Üí ['dis', 'appoint', 'ed']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-E1tDlwZOjc"
   },
   "source": [
    "***WordPiece***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQ7OF9xYXvl1"
   },
   "outputs": [],
   "source": [
    "# Used in: BERT, DistilBERT\n",
    "# Starts with words and breaks unknown ones into chunks using ## for suffixes.\n",
    "# \"playing\" ‚Üí ['play', '##ing']\n",
    "# \"unaffordable\" ‚Üí ['un', '##afford', '##able']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkPMkbIJZfPr"
   },
   "source": [
    "***SentencePiece***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNZsFfURZcII"
   },
   "outputs": [],
   "source": [
    "# Used in: T5, ALBERT, mT5\n",
    "\n",
    "# \"MachineLearning\" ‚Üí ['‚ñÅMachine', 'Learning']\n",
    "# Treats input as raw characters, doesn't require spaces.\n",
    "\n",
    "# Great for multilingual tasks.\n",
    "\n",
    "# Uses \"‚ñÅ\" as a space indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hDu-EeVZ0rw"
   },
   "source": [
    "***Compare how different tokenizers tokenize***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVXvLD2UZmwP"
   },
   "outputs": [],
   "source": [
    "text = \"Transformers are amazing!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKwkUq9iaHEG"
   },
   "source": [
    "| Model   | Tokenizer Type           |\n",
    "| ------- | ------------------------ |\n",
    "| BERT    | WordPiece                |\n",
    "| GPT-2   | Byte-Pair Encoding (BPE) |\n",
    "| T5      | SentencePiece            |\n",
    "| GPT-Neo | Byte-level BPE           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d7KUgHCaVwH"
   },
   "source": [
    "We'll show how each processes the same input, including:\n",
    "\n",
    "Tokens\n",
    "\n",
    "Token IDs\n",
    "\n",
    "Special tokens\n",
    "\n",
    "How unknown words are handled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hN_jC94kaB-W"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLc_upzZaa2U"
   },
   "outputs": [],
   "source": [
    "# Define models\n",
    "tokenizers = {\n",
    "    \"bert-base-uncased\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    \"gpt2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"t5-small\": AutoTokenizer.from_pretrained(\"t5-small\"),\n",
    "    \"EleutherAI/gpt-neo-125M\": AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\"),\n",
    "}\n",
    "\n",
    "text = \"Transformers are amazing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1pNKgaVaqt1",
    "outputId": "faba4f49-8441-4909-801a-fbd283136a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Tokenizer: bert-base-uncased\n",
      "Tokens: ['transformers', 'are', 'amazing', '!']\n",
      "Token IDs: [19081, 2024, 6429, 999]\n",
      "With Special Tokens: [101, 19081, 2024, 6429, 999, 102]\n",
      "Decoded: [CLS] transformers are amazing! [SEP]\n",
      "\n",
      "üîπ Tokenizer: gpt2\n",
      "Tokens: ['Transform', 'ers', 'ƒ†are', 'ƒ†amazing', '!']\n",
      "Token IDs: [41762, 364, 389, 4998, 0]\n",
      "With Special Tokens: [41762, 364, 389, 4998, 0]\n",
      "Decoded: Transformers are amazing!\n",
      "\n",
      "üîπ Tokenizer: t5-small\n",
      "Tokens: ['‚ñÅTransformer', 's', '‚ñÅare', '‚ñÅamazing', '!']\n",
      "Token IDs: [31220, 7, 33, 1237, 55]\n",
      "With Special Tokens: [31220, 7, 33, 1237, 55, 1]\n",
      "Decoded: Transformers are amazing!</s>\n",
      "\n",
      "üîπ Tokenizer: EleutherAI/gpt-neo-125M\n",
      "Tokens: ['Transform', 'ers', 'ƒ†are', 'ƒ†amazing', '!']\n",
      "Token IDs: [41762, 364, 389, 4998, 0]\n",
      "With Special Tokens: [41762, 364, 389, 4998, 0]\n",
      "Decoded: Transformers are amazing!\n"
     ]
    }
   ],
   "source": [
    "for model_name, tokenizer in tokenizers.items():\n",
    "    print(f\"\\nüîπ Tokenizer: {model_name}\")\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    encoded = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Token IDs:\", token_ids)\n",
    "    print(\"With Special Tokens:\", encoded)\n",
    "    print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3PH7NxYcSY1"
   },
   "source": [
    "# *1. BERT (Bidirectional Encoder Representations from Transformers)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abKA4WKieYt5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj6bycLpeQYq"
   },
   "source": [
    "What is BERT?\n",
    "BERT is a transformer-based encoder model that reads both left and right context of a word (bidirectional).\n",
    "It was designed for understanding language, not generating it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wK5SOXpeZa4"
   },
   "source": [
    "Architecture:\n",
    "Uses only the encoder part of the transformer.\n",
    "\n",
    "Trained using Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n",
    "\n",
    "Input = [CLS] sentence [SEP] sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QfZfKZQemUF"
   },
   "source": [
    "| Task                               | Example                            |\n",
    "| ---------------------------------- | ---------------------------------- |\n",
    "| **Text Classification**            | Sentiment analysis, spam detection |\n",
    "| **Named Entity Recognition (NER)** | Highlighting names, places, etc.   |\n",
    "| **Question Answering**             | SQuAD-style tasks                  |\n",
    "| **Text Similarity**                | Sentence pairs                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sZg6xaVerCW"
   },
   "source": [
    "Why Use BERT?\n",
    "\n",
    "Strong at understanding contextual meaning of words.\n",
    "\n",
    "Bidirectional attention gives it deep understanding of syntax and semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwUCLvFOfm79"
   },
   "source": [
    "# . GPT-2 (Generative Pretrained Transformer 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3vd1UNzgDSK"
   },
   "source": [
    "What is GPT-2?\n",
    "\n",
    "GPT-2 is an auto-regressive language model that generates text from left to right, trained to predict the next word given previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccGIk5oRgql2"
   },
   "source": [
    "Architecture:\n",
    "Uses only the decoder part of the transformer.\n",
    "\n",
    "Trained using Causal Language Modeling (CLM).\n",
    "\n",
    "No attention to future tokens (ensures natural, ordered generation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-AGKa7fgzRh"
   },
   "source": [
    "| Task                             | Example                            |\n",
    "| -------------------------------- | ---------------------------------- |\n",
    "| **Text Generation**              | Story, article, or code generation |\n",
    "| **Dialogue Bots**                | Chatbots, assistants               |\n",
    "| **Summarization** *(fine-tuned)* | Generate abstract summaries        |\n",
    "| **Creative Tasks**               | Poems, jokes, completions          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzDhJ3HDg2Yg"
   },
   "source": [
    "Why Use GPT-2?\n",
    "\n",
    "Great at free-form generation and language modeling.\n",
    "\n",
    "Can be fine-tuned for various generative NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXxQF3stg6hG"
   },
   "source": [
    "## T5 (Text-to-Text Transfer Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afgcL4k0hDA2"
   },
   "source": [
    "What is T5?\n",
    "\n",
    "T5 reformulates every NLP task as a text-to-text problem, i.e., both input and output are strings.\n",
    "\n",
    "For example:\n",
    "\n",
    "Input: \"summarize: The news article is ...\"\n",
    "\n",
    "Output: \"The article talks about...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owbck_VyhN_J"
   },
   "source": [
    "Architecture:\n",
    "\n",
    "Uses both encoder and decoder (full transformer).\n",
    "\n",
    "Pretrained on a huge dataset (C4) using a span-masking objective.\n",
    "\n",
    "Trained in a multitask setup: translation, QA, summarization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTJkt1VPhTNq"
   },
   "source": [
    "| Task                   | Example                      |\n",
    "| ---------------------- | ---------------------------- |\n",
    "| **Summarization**      | News article ‚Üí short summary |\n",
    "| **Translation**        | English ‚Üí German             |\n",
    "| **Question Answering** | Text ‚Üí Answer                |\n",
    "| **Classification**     | Sentence ‚Üí label (as text)   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg-ELqdNhW6p"
   },
   "source": [
    "Why Use T5?\n",
    "\n",
    "Extremely flexible (same model for many tasks).\n",
    "\n",
    "Ideal when you want one model to do it all (multi-task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mib0Xm2nhZIM"
   },
   "source": [
    "| Model     | Architecture    | Direction     | Strength               | Common Use                     |\n",
    "| --------- | --------------- | ------------- | ---------------------- | ------------------------------ |\n",
    "| **BERT**  | Encoder-only    | Bidirectional | Understanding          | Classification, QA             |\n",
    "| **GPT-2** | Decoder-only    | Left-to-right | Generation             | Text gen, chatbots             |\n",
    "| **T5**    | Encoder-Decoder | Seq2Seq       | Text-to-text multitask | Summarization, translation, QA |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCxnYasmbBZ8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHyhCZ4Egp6V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeYRLQhVfdXY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

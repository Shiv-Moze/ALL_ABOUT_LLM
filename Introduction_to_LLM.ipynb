{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ6TXTsIanrM"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "93f54af9f9ff45ba8c6475985d448c8a",
      "11c07225408544abbebef46759bf2aef",
      "ff3b181f591e44caaecf8c58a892b3eb",
      "71d7912a89e54fcda5dfe17504aed1bd",
      "b93666a408504b7cb0295b34435e7473",
      "aa1e0bc47a6d42de93064415616f370a",
      "b133760d76bf48408d55f9de0d7fb9e4",
      "4f3831739824446aa24813c0d47bdac9",
      "2472e10ea8a549c8b58d0f67c93f499c",
      "7d1cd4d5de0f4f53985c62773d7503e3",
      "b27b6619b0d040928b6cd7695d03dd69",
      "a14f1ba6d035420698d01744e2b8f327",
      "43d84623af4142c3919317298c703949",
      "6a955323894d4c288d0a16654f059025",
      "3e5fc386a5c84da6806079d522581339",
      "5289d353905e49e8abcbe13c0828517b",
      "023219eb625a41f1b98916aa39101eb0",
      "81058db8e9ae4c8ab6b17e14e80a6b8d",
      "6a0fb65f3fb143b8b66dd717610c9b51",
      "1f9b9d8b6c394250b8e72893cb4d5fa1",
      "29166f61fb5747ca80ebcca37e0dfd82",
      "de1647a691ee4301aaaf99b32a986c3c",
      "d04cd5c3cf754552a4e0a3c5a40e2455",
      "1551496a575a49f2a795d852e8ad5fc6",
      "536e8121da4a4d6892acef9d55823683",
      "d8d6ca84048247a797015c767f8dcbc0",
      "38f9a9629ba44faaa6be6d95e5514f81",
      "25eb610e6dbe490a80ec560307252689",
      "57d7411210aa4b28a5b283f9813ec34d",
      "a677c026c152490b92842fbe74580cdf",
      "1aa3712589cc414eb8370d96f515d0e4",
      "93337f93913f4884baf62b40c519000e",
      "e3d9658208ad43faa8569d18db0f95be",
      "c5ec12aae3a3446f92ca474a9dfd4edd",
      "75ae28c564f147fa8438b304719f068c",
      "6086b4757c824fcd8797b96ee2668325",
      "93d011bad7a54006ac9ecbb4f485fa16",
      "12056f9d624248c896349141edf0cab0",
      "4ca71e5d7f994320ab8c5d824ead9e07",
      "4628da5e10d64c9497352cdec600d7e9",
      "a740162eb43d4b4792629649f97f1a72",
      "5a5233a696144751bd69b4ff8e596e46",
      "ad798b9417554d9b9b68416e009abf32",
      "6bbf6571bd3b439686c74c1b8851cf2f",
      "7d736d6184dc483b93f5f34db27413f5",
      "b594e640ae294b91928368778cef6f35",
      "b7b63f443b474e41932e4edae811818e",
      "76e7b313855d42328929af568b3733d0",
      "7178265ca0394669840690a4ba48c7f1",
      "39f13215fff241539e1ac0ae84156087",
      "02a74de675f44b0f972e995e32c41a84",
      "2b198318b2a74c0d8334c350592d1b89",
      "15e4513de0f24ff2b03a8f5f3b08aaa7",
      "512114a1f0824cc9aa17da93c9fb27ee",
      "68e974a373bb445f92dbddb983c32226",
      "6116a3351a854943b84c8d97d6999748",
      "754b0ab8c1af4ea19ab94006dff0359b",
      "b1b35a25458f42139e5cd0164b1f2ec5",
      "4b7b2b9e61894c50ab80e37c75fcddb2",
      "5a68367e9ad14f52851bddedd6d3b578",
      "a4143a0ee2164917bb4a180cde7d24a0",
      "927fc2fd1be940378bff5e0c455bddf5",
      "748391853f314f7c9c4ef561fcd75e4d",
      "fe1116c8313a4678a7bfe3cc1de1d288",
      "fc1087e6a69243aeb7a46e7518acb183",
      "f179c0c0ac8044f3b1ffe4ad82420a5a",
      "b72b880db01c4fe5911ce0beae9a4d08",
      "fcd73ad101b94fc4918e86acc5777186",
      "4a245bf182804d04bab3347892cbf40c",
      "65759a3efdc440f383a48f896c38a0de",
      "e30544cfb20442e2a5abce6f0d2a4d60",
      "3ed8eaa3962049f58b43eb0e8c2943d0",
      "7342dc2b207d4b85b83e6f35530fa97e",
      "8588f91ac9c14a59877a338df7046738",
      "c088444472794d08aaece5c2a044ccde",
      "1fa03138671d46f48807421df8419293",
      "f695a3779eac49f68c62e3ea6974193f",
      "63315f909dc74ff395b7582917fdb031",
      "4ff53eeb11c84bc2b42c6da7db77d965",
      "6019126eef4a466ebfc7e7c0aad2043d",
      "44191ce9b4604a6a826403ff41a43fb5",
      "e3b1c074db0b4cb5a802e9692f859ed6",
      "6362dc9c998a44e0b4d25646171b340c",
      "f3fa075a4d7446478ee9a2100845aa72",
      "0edcf67d36ca4fa3a825c97720f434ec",
      "fb9f6fc6f3e840c0b172559c49d7eb55",
      "964caa1147ff4b00813745b99dba5467",
      "ee1a691b8d4e454cbd9e1e3bf0001023",
      "673c0f64cdd744e4a51bff09f30ead1e",
      "b15c71d229b6446eb140ea9fe189f063",
      "3dd1818b95e34afcac2ff3ff79933295",
      "f1ea010686d94420aa426ed00a8f06cc",
      "87e8fcfb52084499ace3382989132aea",
      "3a36b0ed12d44e66a0b90fe800b11766",
      "8123b722de6a4c85b615788e1b55803c",
      "3a913f7adb504080bb392bda5878a9b8",
      "482f1090cb424bd7bfdf93735340a1fc",
      "4cf65124ca6045d58143f0a41163adf1",
      "66516c0ec3eb464f88f3fe3e1e0f50a3",
      "e53698efc0b24b48a05cef5da5736a2d",
      "dcc1ccc9a1804fb2895c539ab37d342e",
      "013e1ea9e2d54666bb76660b17a254f9",
      "a9ec2cde036d49ff8846c5757d317d4c",
      "76a7f067e9e6488f82c448a9a482b45a",
      "3b6cfea51d5043c5bb982490a67cefdb",
      "f5b603b2f08a483094d4defdae2a57ac",
      "34733011247f44e394debfa319fbcf88",
      "5f5b8ad77c0e475cb1e892b963652c26",
      "b700490ecfd549ffa5280d990a4f8bdc",
      "26c1b7797c204ce6ba72233d05d0ac2e",
      "135944f17901413f955ea31985b84715",
      "b5e06f9d01f54196946914f19f338bad",
      "d9cc27652c7c4154827841d1374a20b8",
      "e4046c6f42cc4e9791652223a9870ec2",
      "6d2dae42670e498ab7c9b8252156727b",
      "e1590d5b34bb4a61addfdca5a7e863c1",
      "10f5e1e6230e465db035d2abe50fe3f6",
      "e385084f41cd45c5aa9c1d5efb4e8a78",
      "814ece50026343aea93db1902ed03156",
      "86703859ff564945a4a48cfad099d5dc",
      "f99219b2cc974cefa0701fe805ae0bc2",
      "0575e48e8215482d8d859441141f6c91",
      "a5caa6e2b02c4c3d8f0440bca0a265d6",
      "e3046d5dec7b4d8a98dc3d20ee88e4f6",
      "4ad443bfda7145abbb84d8702d31a0c8",
      "809b425d4b3e4b95922d29deba17507f",
      "1f61deea8dbd4094837bc1a674b96dac",
      "cfd2b8be06f94382bd10e58931d010d1",
      "a3cbd86e44f84f048e3a2d9872af2e9e",
      "cbaaf3882d0848739774a5417acb7e82",
      "4fd052ba7ebb45bebc0f178777ba8797",
      "e6a83d5a28494efe9d47ebcd474607c1"
     ]
    },
    "id": "sun5V8fua8Rs",
    "outputId": "c8b32f1e-ac58-454a-8206-49a5affd1b61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f54af9f9ff45ba8c6475985d448c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14f1ba6d035420698d01744e2b8f327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04cd5c3cf754552a4e0a3c5a40e2455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ec12aae3a3446f92ca474a9dfd4edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d736d6184dc483b93f5f34db27413f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6116a3351a854943b84c8d97d6999748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72b880db01c4fe5911ce0beae9a4d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63315f909dc74ff395b7582917fdb031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673c0f64cdd744e4a51bff09f30ead1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53698efc0b24b48a05cef5da5736a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135944f17901413f955ea31985b84715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0575e48e8215482d8d859441141f6c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7FgWCpvbK_z",
    "outputId": "0b1ae724-aa28-4b3b-afc7-b57165dbe182"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7ed793403010>\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "\n",
    "    do_sample=True\n",
    ")\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLmFqjbbcQGh",
    "outputId": "ae18ae5d-df35-4b72-d8c2-5223315845bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' Why did the chicken join the band? Because it had the drumsticks!'}]\n",
      " Why did the chicken join the band? Because it had the drumsticks!\n"
     ]
    }
   ],
   "source": [
    "# Finally, we create our prompt as a user and give it to the model:\n",
    "# The prompt (user input / query)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
    "]\n",
    "\n",
    "# Generate output\n",
    "output = generator(messages)\n",
    "print(output)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668,
     "referenced_widgets": [
      "bf4154dad2b74a3d8fc24f95d2557bb4",
      "53c58f4ba0d145f7ad2323bb5855572f",
      "ed2bca20f3ca40918d5ac6f8325eca8f",
      "a55f73a8b6534a43a6feccce4273e4b2",
      "6bbbd201b0a8453f8b3fc269a2645d00",
      "6aa5757f158b4e258114c74086fa7dec",
      "a4fceb421adb4e308a3c208717777eab",
      "5101ad09f72e4167b4988fbb303b0afb",
      "df74b12529d6496fbca9436e5d834bd5",
      "e248e6703da84b45a91a112ced4f2ec2",
      "6c2757d021ea42ef906cdfad6c015757",
      "e8547f1ea4014f0fa48eeca2e661d782",
      "eac81ce4cc1d40dca86ef3ae25c77c70",
      "7cfddb14dc9041c3ab97ddefd11c2888",
      "c58db3b6ea004e35abe73c469078b773",
      "21efeab73ee64c87ac66336bc826ea4b",
      "63126a75d97a4f808b37fa40ed73f150",
      "d71ee1ab789b4e2d9cb6cd64460d3041",
      "f2cc267fd59547b4b45eaa8cbadcf218",
      "a66a64c03d88420cb957d2fa2c09ddc4",
      "b09c77d423394a4a93bcdde71dd95621",
      "fdf5a194aa7a4a29b7f2535e37ba8280",
      "6080a9df2a804d49b3a4b0620392057f",
      "c4b57d8a2ec949a2bf113bbb2b6a35eb",
      "83ff2227c0184464b89053a8fd00f3de",
      "e53b6b2b224440e4860a338488642c22",
      "b83808fe962c4e3abe2ab9027279a862",
      "c79af7ee29eb411295ece9fb5af326b2",
      "5becf30bb2694a02b91a1c9fb9d44fc7",
      "9e26227e48bb4dd9abf3093ca0486ec0",
      "711f761ab37f4c7f9a3a8a49a9d29a87",
      "ef76fc7d04cc4ad9b628a878c19e7383",
      "7b50772e1e8c467d9c8dfbbc11e40a80",
      "3d80b4a0d6b548828897e0d14485c9d8",
      "31eac1c7801a4684bffc98f85f8c43c9",
      "53cb8d3afab74eb6b57c4ce62b427823",
      "d8f463708ca940b8a6f2273c0ef74fa6",
      "f6a6aac59920474a93a7c3271740ab22",
      "9c7f6ef0ff1e4b5fa24ca3d981534898",
      "8653b3c610164fd9bd2091a335b25c88",
      "a7ef5474ffe24ec68a1397773a40a90c",
      "717682f4966b4642b6a7bc57e43f1054",
      "97d09d3b15fa4572ab3693a33b7cc181",
      "f3a2e2442e014eeab7df43da2d42756f",
      "536a749e222340c09420d766c1c88683",
      "67f5a80279ce4834968c87796816931a",
      "a0d6e5224e0845e193130ea816573762",
      "fc898b291b49441c97297875e674a60e",
      "6e51c2e15cb940eca32a0b22afdedcfe",
      "0f8a684f1ce148788cbb88a3376393c1",
      "ed0cbfce484a46ba8e1b2de2abdfff96",
      "90da5ec9c6094aaf9d7e5220a435714a",
      "9b073933e7554e2eb6dffe618e243dc2",
      "c3758c67a5c7488ba968b8c5e08fe2ac",
      "afcfc7fbaa934ee5b32904a8f6412931",
      "1c46598a235a4f8c98c00d49dd1d346e",
      "a4ac9076fe134488a0af8d63042e424c",
      "bcebc175d64244caa63d077367a8e12d",
      "3428114f17924e8aa94b7e7e7be61e53",
      "f6ac1f0fa90c4cafa861cde330dffea7",
      "de9f18335cb348e18b408f2d69db9a3f",
      "8c2387e0b4d84b16aaee56e67a7d0d5c",
      "b5a0c22d4a9f4097b583f74bd3efd83b",
      "cf124dd2414546699981d35ab6880509",
      "26569df6cef94d9b9443c9c9e9588d09",
      "5400b372728044bd8d331cb33056af05",
      "b18caf19955c466b89b9c3a2555d2e77",
      "716b161d699343809d785df54f2ef417",
      "5b7b179e233044f1bdac5acc6869202d",
      "b4c10b8957554416ae84b778df32fb46",
      "2c9807914d4f4f76863caac5ad75b94f",
      "b00f5ee45fa847e9a2e4a9732cf9a30c",
      "4bfcbc1e618042df9cc447d4c2c9ca17",
      "ad1fdc4974ee4ec09c06ee56fdbd134a",
      "82810c4262a748b786036729b2c507b0",
      "64dd0eb8e6ae483e8b5467008023ee6c",
      "38cb3bb42ea241e58f6832903d1ddcf3",
      "1ba2d639ea1c4306bc37f28ee91bff63",
      "60b9a119e79d48dda2b9437295ad947f",
      "b877ed167336425da687ec22e50e17b3",
      "4e4b50856e5841c386745ffbc3c78b3a",
      "12f1f1eeb18743118225c61e985694fa",
      "9ddaf15ada3a4d9c99f7413d93acca54",
      "e81e1e4b17de427480bd7b690448d951",
      "4d3b1f5d07c148e49a6f45ac75bef055",
      "6f4836a3d9c342a8818a74bb26535c42",
      "386e7084adce465eae81b97a219dd731",
      "50b7fa50febe466193f59b0a46d0a8e7"
     ]
    },
    "id": "uWGX40TJdhbS",
    "outputId": "a17b1d47-bbab-4c90-d1a9-22aec15b43a0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4154dad2b74a3d8fc24f95d2557bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8547f1ea4014f0fa48eeca2e661d782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6080a9df2a804d49b3a4b0620392057f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d80b4a0d6b548828897e0d14485c9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536a749e222340c09420d766c1c88683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c46598a235a4f8c98c00d49dd1d346e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18caf19955c466b89b9c3a2555d2e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba2d639ea1c4306bc37f28ee91bff63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use your brain and body to learn complex topics with little to no instruction. In each lesson, we will present a lesson plan, teach you the skills you need to learn something, and then show you how to apply those skills. This means that you will do most of the learning in class, and then you will practice what you have learned at home.\\n\\nWhat will you learn in this course?\\n\\nIn this course, we will teach you how to use your brain by explaining how it works. We will teach you how to use your brain by explaining how it learns and what you can do to help it learn. You will learn how to use your brain to learn complex topics like calculus and computer science. We will teach you how to use your brain by explaining how it works. We will teach you how to use your brain by explaining how it learns and what you can do to help it learn.\\n\\nWe will teach you how to use your brain by explaining how it learns and what you can do to help it learn. We will teach you how to use your brain by explaining how it works and what you can do to help it learn. We will teach you how to use your brain by explaining how it learns and what you can do to help it learn'},\n",
       " {'generated_text': \"In this course, we will teach you how to make your own paper maché and use it to create a variety of art projects. You'll learn how to make paper mache from scratch, how to use it to create your own artwork, and how to put your art together to create a whole project. You'll learn how to use various forms of paper, such as paper, newsprint, and tissue paper, to create a variety of art projects. You'll also learn how to use the techniques of paper mache to create a variety of works of art. And you'll learn how to put your art together to create a whole project.\\n\\nCourse Outline\\n\\nYou will learn how to make your own paper mache. You will learn how to use the paper mache to create a variety of works of art. You will learn how to use the techniques of paper mache to create a variety of works of art. You will also learn how to put your art together to create a whole project.\\n\\nThe course will have a design element, which is a combination of design and creativity. You will learn how to make your own paper mache. You will learn how to use the techniques of paper mache to create a variety of works of art. You will also learn how to put your art together\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUJUXOS_eetc"
   },
   "outputs": [],
   "source": [
    "# ---------------<steps by steps execution>---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCPWXUH1js0F"
   },
   "outputs": [],
   "source": [
    "# step-by-step, theoretically into how text generation using transformers works,\n",
    "# from input prompt → to tokenizer → model → output text, including what each parameter means and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPmDlTLxj2MD"
   },
   "outputs": [],
   "source": [
    "# 🔹 STEP 1: User Prompt\n",
    "# This is the starting text you give to the model. You’re asking the model to continue writing from this input.\n",
    "prompt = \"Once upon a time\"\n",
    "# This is the context or seed that the model uses to generate the next words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlFJ3wR6kHb0",
    "outputId": "2170bfb4-1126-42c0-a323-4e0e04a5d2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer---------> GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      ")\n",
      "tokens---------> {'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 🔹 STEP 2: Tokenization\n",
    "# 🧾 What:\n",
    "# Tokenization converts the prompt (string) into IDs (integers) the model can understand.\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "prompt = \"Once upon a time\"\n",
    "print(\"tokenizer--------->\",tokenizer)\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "print(\"tokens--------->\",tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4i9TFozJkQa7",
    "outputId": "1920dce4-1e1b-4a0e-cff3-06f1f51feb89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7454, 2402,  257,  640,  286, 1175,   11,  262, 1578, 1829,  373,  262,\n",
      "         691, 1499,  287,  262,  995,  284,  423,  257, 2422, 4931,   13,  383])\n"
     ]
    }
   ],
   "source": [
    "#  STEP 3: Generation (model.generate())\n",
    "# What:\n",
    "# The model now generates new tokens based on your prompt.\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "model.eval()\n",
    "\n",
    "# Generate\n",
    "output = model.generate(\n",
    "    input_ids=tokens[\"input_ids\"],\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# print(output)\n",
    "generated_ids = output[0]\n",
    "print(generated_ids)\n",
    "\n",
    "# These are token IDs for the full generated sequence: original prompt + model output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmjctrtDlTY-",
    "outputId": "19fc200f-72d1-42b8-ddfb-10887394522e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time of war, the United States was the only country in the world to have a military presence. The\n"
     ]
    }
   ],
   "source": [
    "#  STEP 4: Decoding:Convert generated token IDs back into text.\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(generated_text) #This is the final human-readable continuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6WiqS4-mMND",
    "outputId": "9200cbe9-c4b7-4f20-a273-3d31aac2ec94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to solve such problems. We will start by explaining the concept of a matrix. A matrix is a rectangular array of numbers arranged in rows and columns. For example, the matrix\\n\\n[3 1 6] [2 5 7]\\n\\nis called a 3x2 matrix, where 3 is the number of rows, 2 is the number of columns, and [3 1 6] is the matrix itself.\\n\\nNow, let’s see how we can use matrices to solve a system of linear equations. Suppose we have the following system:\\n\\n[3x + 2y + 4z] = 1\\n\\n[x + 2y + 3z] = -2\\n\\n[x + 3y + 4z] = 1\\n\\nWe can solve this system using matrices by first writing it in matrix form:\\n\\nA = [ 3 1 6\\n\\nx + 2y + 3z ]\\n\\n[ x + 2y + 3z ] = -2\\n\\n[ x + 3y + 4z ] = 1\\n\\nNow, we can find the solution to the system using the following formula'},\n",
       " {'generated_text': \"In this course, we will teach you how to write your own computer programs in the C programming language, which is one of the most powerful and important programming languages in the world.\\n\\nC is a general-purpose programming language, and it's a good choice for a wide range of projects. It's a good choice for learning how to program and for learning the basics of programming.\\n\\nC is the standard for writing operating systems and software for embedded systems. It is the language of choice for most computer hardware manufacturers. It is the language of choice for most operating systems.\\n\\nC is the language of choice for most operating systems. It is the language of choice for most embedded systems. C is the language of choice for most operating systems.\"}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 5: Pipeline (Simplified Wrapper)\n",
    "# Internally, this does:\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "# generate(...)\n",
    "\n",
    "# Decoding\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv0qA9gPr4nj"
   },
   "outputs": [],
   "source": [
    "# <---------- Greedy decoding\n",
    "\n",
    "# Top-k sampling\n",
    "\n",
    "# Top-p (nucleus) sampling-------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuaA3zASv7nA"
   },
   "outputs": [],
   "source": [
    "prompt = \"Artificial intelligence is transforming\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKDVCbF1AU3K",
    "outputId": "0d2dcb61-6e5f-4bea-a9c2-f8debfc048bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\", tokenizer=\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvx6JNVNAuKa",
    "outputId": "c031e0d5-de7a-4335-a459-bd9f8da5613b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Nsk96-nv9o3",
    "outputId": "b3286fbf-9068-4017-a591-585d99e9cbd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Greedy: Artificial intelligence is transforming the way we think about our lives.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ⚙️ 1. Greedy Decoding (do_sample=False)\n",
    "\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=False,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id,\n",
    "\n",
    ")\n",
    "\n",
    "print(\"🔹 Greedy:\", output[0]['generated_text'])\n",
    "# 🔹 Greedy: Artificial intelligence is transforming the way we think about our lives.\n",
    "\n",
    "#  Factual, structured\n",
    "\n",
    "#  Slightly boring or repetitive\n",
    "\n",
    "#  Picks the most probable word at each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd_b1lqpwEFR"
   },
   "outputs": [],
   "source": [
    "TRANSFORMERS_VERBOSITY=\"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n46lCz62wibq"
   },
   "outputs": [],
   "source": [
    "# --------<⚙️ 2. Top-k Sampling (do_sample=True, top_k=50)>-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFB0WDw7xWN6",
    "outputId": "8e8e45a1-beb5-4980-8d63-aac5704b7f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔸 Top-k: Artificial intelligence is transforming the world. And, yes, some of your favorite technologies, like cloud computing, will allow you to learn more about the world and the people around\n"
     ]
    }
   ],
   "source": [
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature=1.0,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "print(\" Top-k:\", output[0]['generated_text'])\n",
    "#  Top-k: Artificial intelligence is transforming the world.\n",
    "#  And, yes, some of your favorite technologies, like cloud computing, will allow you to learn more about the world and the people around\n",
    "\n",
    "#  More creative or unexpected\n",
    "\n",
    "#  Less predictable, might hallucinate\n",
    "\n",
    "#  Samples from the top 50 most likely words at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq040fXvxbtx"
   },
   "outputs": [],
   "source": [
    "# --------<⚙️ 3. Top-p Sampling (do_sample=True, top_p=0.9)>-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EeEDuAp6BmJ5",
    "outputId": "5dead952-d71b-47c2-96c0-d1c1fa17fd30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔸 Top-p: Artificial intelligence is transforming the way we think about science. In its wake, artificial intelligence is becoming a reality. Artificial intelligence is revolutionizing the way we think about science.\n"
     ]
    }
   ],
   "source": [
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=1.0,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "print(\" Top-p:\", output[0]['generated_text'])\n",
    "# 🔸 Top-p: Artificial intelligence is transforming the way we think about science.\n",
    "# In its wake, artificial intelligence is becoming a reality. Artificial intelligence is revolutionizing the way we think about science.\n",
    "\n",
    "\n",
    "\n",
    "#  Creative but more focused than top-k\n",
    "\n",
    "# Picks from a dynamic set of top tokens until 90% of total probability is covered\n",
    "\n",
    "#  More adaptive than top-k (the number of candidate tokens changes with context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgShuL4uBrjo"
   },
   "outputs": [],
   "source": [
    "#  Let's walk through a practical example using Hugging Face's pipeline to compare:\n",
    "\n",
    "# top_k only\n",
    "\n",
    "# top_p only\n",
    "\n",
    "# Both top_k + top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Zodhj3VD5tD",
    "outputId": "6ef72a57-9a7a-42c1-8d33-ea6ba32cb541"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TOP-K Sampling (top_k=50)\n",
      "Artificial intelligence is transforming our lives. It is transforming our lives. It is transforming our lives. It is changing our lives. It is transforming our lives. It is transforming our lives. It is transforming our lives. It\n",
      "\n",
      " TOP-P Sampling (top_p=0.9)\n",
      "Artificial intelligence is transforming the way we work and what it means to be human. Artificial intelligence is changing the way we work and what it means to be human.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Combined TOP-K + TOP-P (top_k=50, top_p=0.9)\n",
      "Artificial intelligence is transforming the world into a fully artificial intelligence.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Load text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "# Fixed prompt and seed for consistency\n",
    "prompt = \"Artificial intelligence is transforming\"\n",
    "set_seed(42)\n",
    "\n",
    "print(\"\\n TOP-K Sampling (top_k=50)\")\n",
    "output_top_k = generator(prompt, max_new_tokens=40, do_sample=True, top_k=50,pad_token_id=generator.tokenizer.eos_token_id)\n",
    "print(output_top_k[0]['generated_text'])\n",
    "\n",
    "print(\"\\n TOP-P Sampling (top_p=0.9)\")\n",
    "output_top_p = generator(prompt, max_new_tokens=40, do_sample=True, top_p=0.9,pad_token_id=generator.tokenizer.eos_token_id)\n",
    "print(output_top_p[0]['generated_text'])\n",
    "\n",
    "print(\"\\nCombined TOP-K + TOP-P (top_k=50, top_p=0.9)\")\n",
    "output_combined = generator(prompt, max_new_tokens=40, do_sample=True, top_k=50, top_p=0.9,pad_token_id=generator.tokenizer.eos_token_id)\n",
    "print(output_combined[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LzRoUjUnD76t",
    "outputId": "947418a9-f65d-499b-953b-d3c2cad1a2f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: git@github.com:Shiv-Moze/ALL_ABOUT_LLM.git: No such file or directory\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0fr_jp3WLjLC",
    "outputId": "13b70b1a-ee6b-4b02-cf89-af9ca1f79141"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "DA_frRJSOSTj",
    "outputId": "b93cbfcf-84cc-4e7a-dff4-fdd8f6867f12"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Introduction_to_LLM.ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-32-3938649919.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Introduction_to_LLM.ipynb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'widgets'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'widgets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Introduction_to_LLM.ipynb'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXDpWJ5HT6vu"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAN-uTOpOXLm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
